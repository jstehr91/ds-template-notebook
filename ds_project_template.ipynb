{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Notebook for DS Project\n",
    "Short description of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Notes regarding this notebook\n",
    "This notebook serves as a guideline for a standard data science workflow and contains tips and code snippets to increase efficiency when working on data science projects in Python/jupyter notebooks.\n",
    "\n",
    "Obviously every project is different and this notebook just serves as a general help and guideline. It is work in [progress](https://github.com/jstehr91/ds-template-notebook). \n",
    "\n",
    "### Jupyter Notebook [tips and tricks](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n",
    "**Command mode:** Navigate around notebook\n",
    "* `A`: insert a new cell above the current cell\n",
    "* `B`: insert a new cell below the current cell\n",
    "* `M`: change the current cell to Markdown\n",
    "* `Y`: change the current cell to code\n",
    "* `D (2X)`: delete cell\n",
    "* `Shift + M`: merge multiple selected cells\n",
    "\n",
    "**Edit mode:** Edit individual cells\n",
    "* `Ctrl + Shift + -`: split the current cell where cursor is\n",
    "* `Shift + Tab`: shows docstring documentation of current object\n",
    "* `Ctrl + Shift + -`: split the current cell where cursor is\n",
    "\n",
    "**See all shortcuts:** `H` in command mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions - best practices\n",
    "What we should keep in mind when defining functions:\n",
    "\n",
    "* **Don't repeat yourself** (DRY): whenever we reuse code snippets multiple times, think about creating a function for them\n",
    "* **Do one thing** (DOT): if possible split your functions so that they do only one thing and can be reused in many cases (`load_and_plot()` is not as variable as `load_data()` and a separate `plot_graph()` function)\n",
    "* **Document**: write a docstring to describe the function\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(arg_1, arg_2=42):\n",
    "    \"\"\"Description of what the function does.\n",
    "\n",
    "    Args:\n",
    "      arg_1 (str): Description of arg_1 that can break onto the next line\n",
    "        if needed.\n",
    "      arg_2 (int, optional): Write optional when an argument has a default\n",
    "        value.\n",
    "\n",
    "    Returns:\n",
    "      bool: Optional description of the return value\n",
    "      Extra lines are not indented.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: Include any error types that the function intentionally\n",
    "        raises.\n",
    "\n",
    "    Notes:\n",
    "      See <link> for more info.  \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understand the problem and define the project\n",
    "### Understand the problem\n",
    "* Read about the field\n",
    "* Talk to experts\n",
    "* Recap problem statement\n",
    "\n",
    "### Define the project\n",
    "* Recap information from stakeholders and get confirmation\n",
    "* Set timeline\n",
    "* Define KPIs\n",
    "\n",
    "### Load necessary modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6ac25769a054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# profiling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas_profiling\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# dates and times and time zones and timestamps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "# import usual libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import cufflinks as cf\n",
    "%matplotlib inline\n",
    "\n",
    "# profiling\n",
    "import pandas_profiling as pp\n",
    "\n",
    "# dates and times and time zones and timestamps\n",
    "import datetime as dt\n",
    "import time\n",
    "import pytz\n",
    "\n",
    "# create UUIDs\n",
    "import uuid\n",
    "\n",
    "# set aesthetic parameters\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show installed versions\n",
    "pd.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful cheat-sheets\n",
    "- Pandas: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n",
    "- SQL: https://cdn.sqltutorial.org/wp-content/uploads/2016/04/SQL-cheat-sheet.pdf\n",
    "- Scikit-Learn: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "Or see in GitHub repository (folder `/cheat_sheets/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data acquisition\n",
    "### Datasources\n",
    "Describe the different data sources\n",
    "\n",
    "**Possible sources:**\n",
    "* Internal data bases\n",
    "* Available APIs from used services\n",
    "* Publicly available data sets\n",
    "* Requests to capture certain data\n",
    "\n",
    "**Load helpful libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REST request libraries\n",
    "import requests\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "\n",
    "# SQL database packages\n",
    "import pandabase\n",
    "import psycopg2\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frames\n",
    "df = pd.DataFrame({'col one':[100, 200], 'col two':[300, 400]}) # from dict\n",
    "pd.DataFrame(np.random.rand(4, 8), columns=list('abcdefgh')) # with random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local files\n",
    "train = pd.read_csv('train.csv') # most read_xyz methods can let you choose the separator and the header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API code snippets\n",
    "# define the url for the request\n",
    "url = 'www.api.com'\n",
    "# create a dictionary of headers containing our Authorization header.\n",
    "headers = {\"Authorization\": \"token 1f36137fbbe1602f779300dad26e4c1b7fbab631\"}\n",
    "# define necessary parameters\n",
    "parameters = {\"lat\": 37.78, \"lon\": -122.41}\n",
    "# Make a GET request\n",
    "response = requests.get(url, headers=headers, params=parameters)\n",
    "# get json data from response\n",
    "json_data = response.json()\n",
    "# store it in a data frame\n",
    "data_df = json.normalize(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL code snippets\n",
    "# Connect to Postgres DB\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='template1' user='dbuser' host='localhost' password='dbpass'\")\n",
    "except:\n",
    "    print \"I am unable to connect to the database\"\n",
    "# Define a cursor to work with\n",
    "cur = conn.cursor()\n",
    "# Run a query through the cursor\n",
    "cur.execute(\"\"\"SQL query here\"\"\")\n",
    "# Store the fetched data\n",
    "rows = cur.fetchall()\n",
    "# Close connection to DB\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excursus:** Think about context managers when connecting to databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def database(url):\n",
    "    # set up database connection\n",
    "    db = postgres.connect(url)\n",
    "\n",
    "    yield db\n",
    "\n",
    "    # tear down database connection\n",
    "    db.disconnect()\n",
    "    print('goodbye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in use:\n",
    "url = 'http://website'\n",
    "with database(url) as my_db:\n",
    "    course_list = my_db.execute(\n",
    "      'SELECT * FROM courses'\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exploratory data analysis - clean and understand data\n",
    "- Inspect your data sets and figure out how you can combine them\n",
    "- Identify outliers, missing values, or human error\n",
    "- Ask questions to the specialist to understand all variables and relationships\n",
    "- Extract important variables and leave behind useless variables\n",
    "- Form first hypotheses\n",
    "- Clean your data: Make it homogenous, take care of missing data, remove duplicates in rows or columns, reclassify discrete variables if values are similar\n",
    "- Handle privacy data (tag them and make sure you're compliant)\n",
    "\n",
    "**Helpful resource:** [Data School YT Tutorials](https://www.youtube.com/playlist?list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y) (watch in 2x speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful methods for adjusting pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting columns\n",
    "df = df.rename({'col one':'col_one', 'col two':'col_two'}, axis='columns') # rename some specific columns\n",
    "df.columns = ['col_one', 'col_two'] # rename all columns\n",
    "df.columns = df.columns.str.replace(' ', '_') # format all columns\n",
    "df.add_prefix('X_') # add prefix to all columns (same for suffix)\n",
    "df.loc[:, ::-1] # reverse column order\n",
    "\n",
    "# adjusting rows\n",
    "df.loc[::-1] # reverse row order\n",
    "df.reset_index(drop=True) # reset index and drop old one\n",
    "\n",
    "# dropping columns and rows\n",
    "df.drop(['Names'], axis=1, inplace=True) # drops columns (axis=1) inplace\n",
    "df.isna().sum() # see number of missing values per column; .mean() gives percentage\n",
    "df.dropna(thresh=len(df)*0.9, axis='columns') # dropping columns with more than 10% missing values\n",
    "\n",
    "# sort series or data frame\n",
    "df.sort_values(['col1', 'col2'], ascending=True) # sort data frame according to multiple columns\n",
    "\n",
    "# access certain cells in a data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful methods for first insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape() # show no. of rows and columns\n",
    "df.info() # show columns\n",
    "df.head/sample/tail() # show sample columns\n",
    "df.columns # show columns of data set\n",
    "df.nunique(axis=0) # shows no. of unique values per column\n",
    "df.describe().apply(lambda s: s.apply(lambda x: format(x, 'f'))) # summarizes the count, mean, standard deviation, min, and max for numeric variables (following code formats data for better reading)\n",
    "\n",
    "# forming ProfileReport and save as output.html file \n",
    "profile = pp.ProfileReport(df) \n",
    "profile.to_file(\"output.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful methods for cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reclassify: if row.column in value_list return value -> apply to column\n",
    "df.drop([columns], axis=1) # drop (duplicated) columns\n",
    "\n",
    "#Drop columns with more than x % NA values:\n",
    "NA_val = df_cleaned.isna().sum()\n",
    "def na_filter(na, threshold = .4): # only select variables that pass the threshold\n",
    "    col_pass = []\n",
    "    for i in na.keys():\n",
    "        if na[i]/df_cleaned.shape[0]<threshold:\n",
    "            col_pass.append(i)\n",
    "    return col_pass\n",
    "df_cleaned = df_cleaned[na_filter(NA_val)]\n",
    "df_cleaned.columns\n",
    "\n",
    "df[df[column] >/</==/.between(low, high)] # remove outliers: \n",
    "df.dropna(axis=0) # remove rows with Null values\n",
    "\n",
    "# convert continuous values into categorical values\n",
    "pd.cut(titanic.Age, bins=[0, 18, 25, 99], labels=['child', 'young adult', 'adult']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful methods on finding relationships between attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print correlation heatmap:\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True)\n",
    "\n",
    "# scatterplot to display relationship of two variables:\n",
    "df.plot(kind='scatter', x=col1, y=col2)\n",
    "\n",
    "# combine histogram per attribute and scatterplot for all relationships:\n",
    "sns.pairplot(df)\n",
    "\n",
    "# explore a single variable: \n",
    "df[col].plot(kind='hist', bins=123) #histogram\n",
    "df.boxplot(col) # boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Enrich data set with additional data\n",
    "- Get most value out of the data set by combining data, clean time-based attributes\n",
    "- Analyze relationships between the variables\n",
    "- Try to not reinforce bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build helpful visualizations for communication\n",
    "- Visualization is the best way to explore and communicate your findings\n",
    "- Effective way to quickly communicate a lot of information in a short period of time\n",
    "- Make the visualizations interactive and intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Get predictive - machine learning\n",
    "- Machine learning algorithms can help you go a step further into getting insights and predicting future trends\n",
    "- Unsupervised clustering algorithms can build models to uncover trends in the data that were not distinguishable in graphs and stats\n",
    "- Supervised algorithms can predict future trends\n",
    "- Once a model is deployed, we need to operationalize it - it should not stay unused on the shelves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasstehr/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/jonasstehr/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'parameter': [a, b, c]}\n",
    "\n",
    "# create the grid search instance\n",
    "grid = GridSearchCV(model_instance,param_grid,refit=True,verbose=3)\n",
    "# fit the model via grid search and find the best combination\n",
    "grid.fit(X_train,y_train)\n",
    "# show the best parameter combination\n",
    "grid.best_params_\n",
    "# predict the target values\n",
    "grid_predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Iterate and maintain\n",
    "- Prove the effectiveness of the project as fast as possible to justify the project\n",
    "- Maintain the model as the input and environment can change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
