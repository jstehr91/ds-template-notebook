{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template [Notebook](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/) for DS Project\n",
    "Short description of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understand the problem and define the project\n",
    "### Understand the problem\n",
    "* Read about the field\n",
    "* Talk to experts\n",
    "* Recap problem statement\n",
    "\n",
    "### Define the project\n",
    "* Recap stakeholders\n",
    "* Set timeline\n",
    "* Define KPIs\n",
    "\n",
    "### Load necessary modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import usual libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import cufflinks as cf\n",
    "%matplotlib inline\n",
    "\n",
    "# REST request libraries\n",
    "import requests\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "\n",
    "# SQL database packages\n",
    "import pandabase\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "\n",
    "# profiling\n",
    "import pandas_profiling as pp\n",
    "\n",
    "# dates and times and time zones and timestamps\n",
    "import datetime as dt\n",
    "import time\n",
    "import pytz\n",
    "\n",
    "# create UUIDs\n",
    "import uuid\n",
    "\n",
    "# set aesthetic parameters\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful cheat-sheets\n",
    "- Pandas: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n",
    "- SQL: https://cdn.sqltutorial.org/wp-content/uploads/2016/04/SQL-cheat-sheet.pdf\n",
    "- Scikit-Learn: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data acquisition\n",
    "### Datasources\n",
    "Describe the different data sources\n",
    "\n",
    "**Possible sources:**\n",
    "* Internal data bases\n",
    "* Available APIs from used services\n",
    "* Publicly available data sets\n",
    "* Requests to capture certain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local files\n",
    "train = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API code snippets\n",
    "# define the url for the request\n",
    "url = 'www.api.com'\n",
    "# create a dictionary of headers containing our Authorization header.\n",
    "headers = {\"Authorization\": \"token 1f36137fbbe1602f779300dad26e4c1b7fbab631\"}\n",
    "# define necessary parameters\n",
    "parameters = {\"lat\": 37.78, \"lon\": -122.41}\n",
    "# Make a GET request\n",
    "response = requests.get(url, headers=headers, params=parameters)\n",
    "# get json data from response\n",
    "json_data = response.json()\n",
    "# store it in a data frame\n",
    "data_df = json.normalize(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL code snippets\n",
    "# Connect to Postgres DB\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='template1' user='dbuser' host='localhost' password='dbpass'\")\n",
    "except:\n",
    "    print \"I am unable to connect to the database\"\n",
    "# Define a cursor to work with\n",
    "cur = conn.cursor()\n",
    "# Run a query through the cursor\n",
    "cur.execute(\"\"\"SQL query here\"\"\")\n",
    "# Store the fetched data\n",
    "rows = cur.fetchall()\n",
    "# Close connection to DB\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exploratory data analysis - clean and understand data\n",
    "- Inspect your data sets and figure out how you can combine them\n",
    "- Identify outliers, missing values, or human error\n",
    "- Ask questions to the specialist to understand all variables and relationships\n",
    "- Extract important variables and leave behind useless variables\n",
    "- Form first hypotheses\n",
    "- Clean your data: Make it homogenous, take care of missing data, remove duplicates in rows or columns, reclassify discrete variables if values are similar\n",
    "- Handle privacy data (tag them and make sure you're compliant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful methods for first insights\n",
    "df.shape() # show no. of rows and columns\n",
    "df.info() # show columns\n",
    "df.head/sample/tail() # show sample columns\n",
    "df.columns # show columns of data set\n",
    "df.nunique(axis=0) # shows no. of unique values per column\n",
    "df.describe().apply(lambda s: s.apply(lambda x: format(x, 'f'))) # summarizes the count, mean, standard deviation, min, and max for numeric variables (following code formats data for better reading)\n",
    "\n",
    "# forming ProfileReport and save as output.html file \n",
    "profile = pp.ProfileReport(df) \n",
    "profile.to_file(\"output.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful methods for cleaning data:\n",
    "\n",
    "# Reclassify: if row.column in value_list return value -> apply to column\n",
    "df.drop([columns], axis=1) # drop (duplicated) columns: \n",
    "\n",
    "#Drop columns with more than x % NA values:\n",
    "NA_val = df_cleaned.isna().sum()\n",
    "def na_filter(na, threshold = .4): # only select variables that pass the threshold\n",
    "    col_pass = []\n",
    "    for i in na.keys():\n",
    "        if na[i]/df_cleaned.shape[0]<threshold:\n",
    "            col_pass.append(i)\n",
    "    return col_pass\n",
    "df_cleaned = df_cleaned[na_filter(NA_val)]\n",
    "df_cleaned.columns\n",
    "\n",
    "df[df[column] >/</==/.between(low, high)] # remove outliers: \n",
    "df.dropna(axis=0) # remove rows with Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful methods on finding relationships between attributes:\n",
    "\n",
    "# print correlation heatmap:\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True)\n",
    "\n",
    "# scatterplot to display relationship of two variables:\n",
    "df.plot(kind='scatter', x=col1, y=col2)\n",
    "\n",
    "# combine histogram per attribute and scatterplot for all relationships:\n",
    "sns.pairplot(df)\n",
    "\n",
    "# explore a single variable: \n",
    "df[col].plot(kind='hist', bins=123) #histogram\n",
    "df.boxplot(col) # boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Enrich data set with additional data\n",
    "- Get most value out of the data set by combining data, clean time-based attributes\n",
    "- Analyze relationships between the variables\n",
    "- Try to not reinforce bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build helpful visualizations for communication\n",
    "- Visualization is the best way to explore and communicate your findings\n",
    "- Effective way to quickly communicate a lot of information in a short period of time\n",
    "- Make the visualizations interactive and intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Get predictive - machine learning\n",
    "- Machine learning algorithms can help you go a step further into getting insights and predicting future trends\n",
    "- Unsupervised clustering algorithms can build models to uncover trends in the data that were not distinguishable in graphs and stats\n",
    "- Supervised algorithms can predict future trends\n",
    "- Once a model is deployed, we need to operationalize it - it should not stay unused on the shelves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Iterate and maintain\n",
    "- Prove the effectiveness of the project as fast as possible to justify the project\n",
    "- Maintain the model as the input and environment can change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
